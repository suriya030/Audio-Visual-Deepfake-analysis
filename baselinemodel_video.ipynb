{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset,Subset\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# from adopt import ADOPT\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True # only applies to CUDA convolutional operation.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalConv1D(nn.Module):\n",
    "    \"\"\"Simple temporal conv block for feature alignment\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,dp=0.0):\n",
    "        super(TemporalConv1D, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                               padding=padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## cross attention #######\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention between two feature streams\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=16,dp=0.0):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads,\n",
    "                                                    dropout=dp, batch_first=True)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## MHSA ########\n",
    "class MHSA(nn.Module):\n",
    "    \"\"\"Multi-Head Self Attention\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=16,dp=0.0):\n",
    "        super(MHSA, self).__init__()\n",
    "        self.mhsa = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads,dropout=dp, \n",
    "                                          batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.mhsa(x, x, x)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Fusion model v4 ########\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, swin_feat_dim=(50, 1024)):\n",
    "        \n",
    "        super(FusionModel, self).__init__()\n",
    "\n",
    "        \n",
    "        # Apply custom weight initialization\n",
    "        self.apply(self.kaiming_initialize)\n",
    "    \n",
    "    def kaiming_initialize(self,layer):\n",
    "        \"\"\"Apply Kaiming initialization for all relevant layers, including MultiHeadAttention.\"\"\"\n",
    "        if isinstance(layer, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "        elif isinstance(layer, nn.MultiheadAttention):\n",
    "            # Initialize in_proj_weight and in_proj_bias\n",
    "            nn.init.kaiming_normal_(layer.in_proj_weight, mode='fan_in', nonlinearity='relu')\n",
    "            if layer.in_proj_bias is not None:\n",
    "                nn.init.constant_(layer.in_proj_bias, 0)\n",
    "            # Initialize out_proj weight and bias\n",
    "            nn.init.kaiming_normal_(layer.out_proj.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if layer.out_proj.bias is not None:\n",
    "                nn.init.constant_(layer.out_proj.bias, 0)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, swin_features ):\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(swin_file_path,labels_file_path):\n",
    "     \n",
    "    swin_features = np.load(swin_file_path)\n",
    "    labels = np.load(labels_file_path)\n",
    "\n",
    "    swin_features = torch.from_numpy( swin_features).float()\n",
    "    labels = torch.from_numpy( labels ).long()\n",
    "    \n",
    "    return swin_features,labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCCT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
